{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97024cf4-c930-4aa1-a13f-ecdcc2462afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5404d6b6-6ce1-48df-8999-7da3fb082662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be defined by the drop down in the UI\n",
    "subgroup1 = \"woman\"\n",
    "subgroup2 = \"man\"\n",
    "subgroup3 = \"non-binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bf08cc-e18f-49fc-bae7-f03a9a492baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"c4\", \"en\", split= \"train\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca6ee78-6f96-45e2-8dad-ec745c953d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Just taking the first 10000 instances.\n"
     ]
    }
   ],
   "source": [
    "grab_n = 10000\n",
    "# For streaming data\n",
    "print('Note: Just taking the first %s instances.' % grab_n)\n",
    "data_head = data.take(grab_n)\n",
    "#data_head = [[\"there is a woman with a hairbrush\"],[\"there is a woman with a hairbrush\"],[\"there is a woman with a hairbrush\"],[\"there is a man with a dog\"],[\"there is a man with a dog\"]]\n",
    "df = pd.DataFrame(data_head, columns=[\"text\"])\n",
    "# If not streaming, use:\n",
    "#df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d4d0c8-fc7c-4e99-8207-2e93d1c9b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab_frequencies(df):\n",
    "    \"\"\"\n",
    "    Based on an input pandas DataFrame with a 'text' column, \n",
    "    this function will count the occurrences of all words\n",
    "    with a frequency higher than 'cutoff' and will return another DataFrame\n",
    "    with the rows corresponding to the different vocabulary words\n",
    "    and the column to the count count of that word.\n",
    "    \"\"\"\n",
    "    # Move this up as a constant in larger code.\n",
    "    batch_size = 10\n",
    "    \n",
    "    # We do this to calculate per-word statistics\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # Regex for pulling out single words\n",
    "    cvec = CountVectorizer(token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", lowercase=True)\n",
    "    \n",
    "    # We also do this because we need to have the tokenization per sentence \n",
    "    # so that we can look at co-occurrences of words across sentences for nPMI calculation\n",
    "    sent_tokenizer = cvec.build_tokenizer()\n",
    "    df['tokenized'] = df.text.apply(sent_tokenizer)\n",
    "    \n",
    "    # Fast calculation of single word counts\n",
    "    cvec.fit(df.text)\n",
    "    document_matrix = cvec.transform(df.text)\n",
    "    batches = np.linspace(0, df.shape[0], batch_size).astype(int)\n",
    "    i = 0\n",
    "    tf = []\n",
    "    while i < len(batches) - 1:\n",
    "        batch_result = np.sum(document_matrix[batches[i]:batches[i+1]].toarray(), axis=0)\n",
    "        tf.append(batch_result)\n",
    "        i += 1\n",
    "    term_freq_df = pd.DataFrame([np.sum(tf, axis=0)], columns=cvec.get_feature_names()).transpose()\n",
    "    \n",
    "    # Now organize everything into the dataframes\n",
    "    term_freq_df.columns = ['count']\n",
    "    term_freq_df.index.name = 'word'\n",
    "    sorted_term_freq_df = pd.DataFrame(term_freq_df.sort_values(by='count', ascending=False)['count'])\n",
    "    return sorted_term_freq_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b291a4-088e-463e-8a5e-326bcb0d9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       count  proportion\n",
      "word                    \n",
      "the   186019    0.050628\n",
      "and   107893    0.029365\n",
      "to    103090    0.028058\n",
      "of     89417    0.024336\n",
      "a      81307    0.022129\n",
      "             count    proportion\n",
      "word                            \n",
      "interestel       1  2.721674e-07\n",
      "interethnic      1  2.721674e-07\n",
      "interfaced       1  2.721674e-07\n",
      "interfacing      1  2.721674e-07\n",
      "ðŒ¼ðŒ¿ðŒ½ðŒ³ðƒ            1  2.721674e-07\n"
     ]
    }
   ],
   "source": [
    "term_df, df = count_vocab_frequencies(df)\n",
    "# p(word).  Note that multiple occurrences of a word in a sentence increases its probability.\n",
    "# We may want to do something about that.\n",
    "term_df['proportion'] = term_df['count']/float(sum(term_df['count']))\n",
    "# Sanity check\n",
    "print(term_df.head())\n",
    "print(term_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8202c12e-6d02-4dbc-a896-c1ce1d3b34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PMI(df_coo, subgroup):\n",
    "    # PMI(x;y) = h(y) - h(y|x)\n",
    "    #          = h(subgroup) - h(subgroup|word)\n",
    "    #          = log (p(subgroup|word) / p(subgroup))\n",
    "    # nPMI additionally divides by -log(p(x,y)) = -log(p(x|y)p(y))\n",
    "\n",
    "    # Calculation of p(subgroup)\n",
    "    subgroup_prob = term_df.loc[subgroup]['proportion']\n",
    "    # Apply a function to all words to calculate log p(subgroup|word)\n",
    "    # The word is indexed by mlb.classes_ ; \n",
    "    # we pull out the word using the mlb.classes_ index and then get its count using our main term_df\n",
    "    pmi_df = pd.DataFrame(df_coo.apply(lambda x: np.log(x.values/term_df.loc[mlb.classes_[x.index]]['count']/subgroup_prob)))\n",
    "    # If all went well, this will be correlated with high frequency words\n",
    "    # Until normalizing\n",
    "    return pmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92c6626f-ce6d-4630-a92b-ba89882743ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nPMI(pmi_df, df_coo):\n",
    "    normalize_df = pd.DataFrame(df_coo.apply(lambda x: -np.log(x.values/term_df.loc[mlb.classes_[x.index]]['count'] * term_df.loc[mlb.classes_[x.index]]['proportion'])))\n",
    "    npmi_df = pmi_df/normalize_df\n",
    "    return npmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7207c16-7f2c-470a-b9d7-6a52e03b6a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/margaretmitchell/opt/anaconda3/lib/python3.8/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Makes a sparse vector (shape: # sentences x # words),\n",
    "# with the count of each word per sentence.\n",
    "#    the a cat poop woman Blind\n",
    "# 0 \n",
    "# 1\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_mlb = pd.DataFrame(mlb.fit_transform(df['tokenized']))\n",
    "npmi_df_pair = pd.DataFrame(columns=[subgroup1, subgroup2])\n",
    "pmi_df_pair = pd.DataFrame(columns=[subgroup1, subgroup2])\n",
    "for subgroup in (subgroup1, subgroup2):\n",
    "    # Index of the subgroup word in the sparse vector\n",
    "    subgroup_idx = np.where(mlb.classes_ == subgroup)[0][0]\n",
    "    # Dataframe for the subgroup (with counts)\n",
    "    df_subgroup = df_mlb.iloc[:, subgroup_idx]\n",
    "    # Create cooccurence matrix for the given subgroup and all other words.\n",
    "    # Note it also includes the word itself, so that count should be subtracted \n",
    "    # (the word will always co-occur with itself)\n",
    "    df_coo = pd.DataFrame(df_mlb.T.dot(df_subgroup))#.drop(index=subgroup_idx, axis=1)\n",
    "    pmi_df = get_PMI(df_coo, subgroup)\n",
    "    pmi_df_pair[subgroup] = pmi_df\n",
    "    npmi_df = get_nPMI(pmi_df, df_coo)\n",
    "    npmi_df_pair[subgroup] = npmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d670174e-61b4-4af8-9203-540ec928dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# woman - man: If it's negative, it's man-biased; if it's positive, it's woman positive.\n",
    "npmi_bias = pd.DataFrame(npmi_df_pair[subgroup1] - npmi_df_pair[subgroup2])\n",
    "pmi_bias = pd.DataFrame(pmi_df_pair[subgroup1] - pmi_df_pair[subgroup2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986c4cb3-ba23-4bcd-8f08-1d17c01a7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that only occur with one or the other -- needed for PMI\n",
    "s1_only_words = pmi_bias[pmi_bias[0].values==np.inf]\n",
    "s2_only_words = pmi_bias[pmi_bias[0].values==-np.inf]\n",
    "\n",
    "# Filter\n",
    "npmi_bias_filtered = npmi_bias.dropna() # [(np.inf > npmi_bias[0]) & (npmi_bias[0] > -np.inf)].sort_values(by=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63bedd3c-6ae4-44e5-a063-80d943efbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dd94085-e77e-4fe3-a955-1821f54426de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@50, the man bias is:\t0.28\n",
      "@50, the woman bias is:\t2.54\n"
     ]
    }
   ],
   "source": [
    "print(\"@%s, the %s bias is:\\t%.2f\" % (n, subgroup2, np.abs(sum(npmi_bias_filtered[:n].values))))\n",
    "print(\"@%s, the %s bias is:\\t%.2f\" % (n, subgroup1, sum(npmi_bias_filtered[-n:].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ec116a-29d5-4b88-8aac-cd3ced1aa8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 most man-biased words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>-0.260967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pure</th>\n",
       "      <td>-0.248211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foot</th>\n",
       "      <td>-0.243202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>failed</th>\n",
       "      <td>-0.224456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>league</th>\n",
       "      <td>-0.211324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decade</th>\n",
       "      <td>-0.205872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minds</th>\n",
       "      <td>-0.204651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>squad</th>\n",
       "      <td>-0.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leg</th>\n",
       "      <td>-0.199634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisely</th>\n",
       "      <td>-0.190910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>route</th>\n",
       "      <td>-0.190002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gained</th>\n",
       "      <td>-0.189576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin</th>\n",
       "      <td>-0.186229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desires</th>\n",
       "      <td>-0.183848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>premier</th>\n",
       "      <td>-0.183546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blocked</th>\n",
       "      <td>-0.181568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wave</th>\n",
       "      <td>-0.181540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actor</th>\n",
       "      <td>-0.179628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versions</th>\n",
       "      <td>-0.179389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thinks</th>\n",
       "      <td>-0.178664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returns</th>\n",
       "      <td>-0.176453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duties</th>\n",
       "      <td>-0.175250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instantly</th>\n",
       "      <td>-0.175108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pull</th>\n",
       "      <td>-0.174607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drinks</th>\n",
       "      <td>-0.174286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steady</th>\n",
       "      <td>-0.174023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>captured</th>\n",
       "      <td>-0.169768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ban</th>\n",
       "      <td>-0.169372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remains</th>\n",
       "      <td>-0.168433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supposedly</th>\n",
       "      <td>-0.167906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kill</th>\n",
       "      <td>-0.167006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>-0.166815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encounter</th>\n",
       "      <td>-0.166746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>additionally</th>\n",
       "      <td>-0.166446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gotta</th>\n",
       "      <td>-0.165953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-0.165687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deny</th>\n",
       "      <td>-0.165598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incident</th>\n",
       "      <td>-0.165205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milk</th>\n",
       "      <td>-0.164745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beating</th>\n",
       "      <td>-0.163666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verse</th>\n",
       "      <td>-0.163373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bruce</th>\n",
       "      <td>-0.163086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delay</th>\n",
       "      <td>-0.162617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>-0.162613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandoned</th>\n",
       "      <td>-0.161499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.161429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>-0.161331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish</th>\n",
       "      <td>-0.160348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dramatically</th>\n",
       "      <td>-0.159881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cap</th>\n",
       "      <td>-0.159172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0\n",
       "word                  \n",
       "man          -0.260967\n",
       "pure         -0.248211\n",
       "foot         -0.243202\n",
       "failed       -0.224456\n",
       "league       -0.211324\n",
       "decade       -0.205872\n",
       "minds        -0.204651\n",
       "squad        -0.204100\n",
       "leg          -0.199634\n",
       "precisely    -0.190910\n",
       "route        -0.190002\n",
       "gained       -0.189576\n",
       "origin       -0.186229\n",
       "desires      -0.183848\n",
       "premier      -0.183546\n",
       "blocked      -0.181568\n",
       "wave         -0.181540\n",
       "actor        -0.179628\n",
       "versions     -0.179389\n",
       "thinks       -0.178664\n",
       "returns      -0.176453\n",
       "duties       -0.175250\n",
       "instantly    -0.175108\n",
       "pull         -0.174607\n",
       "drinks       -0.174286\n",
       "steady       -0.174023\n",
       "captured     -0.169768\n",
       "ban          -0.169372\n",
       "remains      -0.168433\n",
       "supposedly   -0.167906\n",
       "kill         -0.167006\n",
       "compare      -0.166815\n",
       "encounter    -0.166746\n",
       "additionally -0.166446\n",
       "gotta        -0.165953\n",
       "1997         -0.165687\n",
       "deny         -0.165598\n",
       "incident     -0.165205\n",
       "milk         -0.164745\n",
       "beating      -0.163666\n",
       "verse        -0.163373\n",
       "bruce        -0.163086\n",
       "delay        -0.162617\n",
       "software     -0.162613\n",
       "abandoned    -0.161499\n",
       "38           -0.161429\n",
       "length       -0.161331\n",
       "fish         -0.160348\n",
       "dramatically -0.159881\n",
       "cap          -0.159172"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top %s most %s-biased words\" % (n, subgroup2))\n",
    "npmi_bias_filtered.sort_values(by=[0], ascending=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bcb965a-4709-4d03-b682-bc8c1af41175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 most woman-biased words\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0.296502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quit</th>\n",
       "      <td>0.252845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swedish</th>\n",
       "      <td>0.238161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complement</th>\n",
       "      <td>0.233675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vitamins</th>\n",
       "      <td>0.232567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>representation</th>\n",
       "      <td>0.229159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miraculously</th>\n",
       "      <td>0.223465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coaster</th>\n",
       "      <td>0.219417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activism</th>\n",
       "      <td>0.219417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shoots</th>\n",
       "      <td>0.215766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>childbirth</th>\n",
       "      <td>0.215424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grooming</th>\n",
       "      <td>0.215424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>murray</th>\n",
       "      <td>0.212285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strengthening</th>\n",
       "      <td>0.210629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mai</th>\n",
       "      <td>0.209576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alice</th>\n",
       "      <td>0.209439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>malaysia</th>\n",
       "      <td>0.206596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workplace</th>\n",
       "      <td>0.205919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flights</th>\n",
       "      <td>0.202572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpless</th>\n",
       "      <td>0.201522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desperately</th>\n",
       "      <td>0.195599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scourge</th>\n",
       "      <td>0.193043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fearing</th>\n",
       "      <td>0.190395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alternately</th>\n",
       "      <td>0.189450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preconceived</th>\n",
       "      <td>0.189450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>births</th>\n",
       "      <td>0.188651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatter</th>\n",
       "      <td>0.188651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>testified</th>\n",
       "      <td>0.187958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnancies</th>\n",
       "      <td>0.187958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>courageous</th>\n",
       "      <td>0.186801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clad</th>\n",
       "      <td>0.186307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>honorary</th>\n",
       "      <td>0.185856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dumb</th>\n",
       "      <td>0.185441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immensely</th>\n",
       "      <td>0.185057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lump</th>\n",
       "      <td>0.184699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>booster</th>\n",
       "      <td>0.184699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chester</th>\n",
       "      <td>0.184365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attire</th>\n",
       "      <td>0.184365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brighten</th>\n",
       "      <td>0.184365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rituals</th>\n",
       "      <td>0.184365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scarf</th>\n",
       "      <td>0.184051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cornell</th>\n",
       "      <td>0.183754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>downs</th>\n",
       "      <td>0.183474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epidemic</th>\n",
       "      <td>0.183208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overlap</th>\n",
       "      <td>0.183208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chambers</th>\n",
       "      <td>0.182714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andrea</th>\n",
       "      <td>0.182483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garcia</th>\n",
       "      <td>0.182483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doll</th>\n",
       "      <td>0.182263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investigator</th>\n",
       "      <td>0.182051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0\n",
       "word                    \n",
       "woman           0.296502\n",
       "quit            0.252845\n",
       "swedish         0.238161\n",
       "complement      0.233675\n",
       "vitamins        0.232567\n",
       "representation  0.229159\n",
       "miraculously    0.223465\n",
       "coaster         0.219417\n",
       "activism        0.219417\n",
       "shoots          0.215766\n",
       "childbirth      0.215424\n",
       "grooming        0.215424\n",
       "murray          0.212285\n",
       "strengthening   0.210629\n",
       "mai             0.209576\n",
       "alice           0.209439\n",
       "malaysia        0.206596\n",
       "workplace       0.205919\n",
       "flights         0.202572\n",
       "helpless        0.201522\n",
       "desperately     0.195599\n",
       "scourge         0.193043\n",
       "fearing         0.190395\n",
       "alternately     0.189450\n",
       "preconceived    0.189450\n",
       "births          0.188651\n",
       "chatter         0.188651\n",
       "testified       0.187958\n",
       "pregnancies     0.187958\n",
       "courageous      0.186801\n",
       "clad            0.186307\n",
       "honorary        0.185856\n",
       "dumb            0.185441\n",
       "immensely       0.185057\n",
       "lump            0.184699\n",
       "booster         0.184699\n",
       "chester         0.184365\n",
       "attire          0.184365\n",
       "brighten        0.184365\n",
       "rituals         0.184365\n",
       "scarf           0.184051\n",
       "cornell         0.183754\n",
       "downs           0.183474\n",
       "epidemic        0.183208\n",
       "overlap         0.183208\n",
       "chambers        0.182714\n",
       "andrea          0.182483\n",
       "garcia          0.182483\n",
       "doll            0.182263\n",
       "investigator    0.182051"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top %s most %s-biased words\" % (n,subgroup1))\n",
    "npmi_bias_filtered.sort_values(by=[0], ascending=True)[-n:].sort_values(by=[0], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
