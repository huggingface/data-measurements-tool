{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97024cf4-c930-4aa1-a13f-ecdcc2462afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755fb84-15ad-40d1-ae18-a5ff5fb30278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m nltk.downloader wordnet\n",
    "#!python -m nltk.downloader omw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5404d6b6-6ce1-48df-8999-7da3fb082662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be defined by the drop down in the UI\n",
    "subgroup1 = \"woman\"\n",
    "subgroup2 = \"man\"\n",
    "subgroup3 = \"non-binary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bf08cc-e18f-49fc-bae7-f03a9a492baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"c4\", \"en\", split= \"train\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca6ee78-6f96-45e2-8dad-ec745c953d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Just taking the first 10000 instances.\n"
     ]
    }
   ],
   "source": [
    "grab_n = 10000\n",
    "# For streaming data\n",
    "print('Note: Just taking the first %s instances.' % grab_n)\n",
    "data_head = data.take(grab_n)\n",
    "#data_head = [[\"there is a woman with a hairbrush\"],[\"there is a woman with a hairbrush\"],[\"there is a woman with a hairbrush\"],[\"there is a man with a dog\"],[\"there is a man with a dog\"]]\n",
    "df = pd.DataFrame(data_head, columns=[\"text\"])\n",
    "# If not streaming, use:\n",
    "#df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d4d0c8-fc7c-4e99-8207-2e93d1c9b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab_frequencies(df):\n",
    "    \"\"\"\n",
    "    Based on an input pandas DataFrame with a 'text' column, \n",
    "    this function will count the occurrences of all words\n",
    "    with a frequency higher than 'cutoff' and will return another DataFrame\n",
    "    with the rows corresponding to the different vocabulary words\n",
    "    and the column to the count count of that word.\n",
    "    \"\"\"\n",
    "    # Move this up as a constant in larger code.\n",
    "    batch_size = 10\n",
    "    \n",
    "    # We do this to calculate per-word statistics\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # Regex for pulling out single words\n",
    "    cvec = CountVectorizer(token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\", lowercase=True)\n",
    "    \n",
    "    # We also do this because we need to have the tokenization per sentence \n",
    "    # so that we can look at co-occurrences of words across sentences for nPMI calculation\n",
    "    sent_tokenizer = cvec.build_tokenizer()\n",
    "    df['tokenized'] = df.text.apply(sent_tokenizer)\n",
    "    \n",
    "    # Fast calculation of single word counts\n",
    "    cvec.fit(df.text)\n",
    "    document_matrix = cvec.transform(df.text)\n",
    "    batches = np.linspace(0, df.shape[0], batch_size).astype(int)\n",
    "    i = 0\n",
    "    tf = []\n",
    "    while i < len(batches) - 1:\n",
    "        batch_result = np.sum(document_matrix[batches[i]:batches[i+1]].toarray(), axis=0)\n",
    "        tf.append(batch_result)\n",
    "        i += 1\n",
    "    term_freq_df = pd.DataFrame([np.sum(tf, axis=0)], columns=cvec.get_feature_names()).transpose()\n",
    "    \n",
    "    # Now organize everything into the dataframes\n",
    "    term_freq_df.columns = ['count']\n",
    "    term_freq_df.index.name = 'word'\n",
    "    sorted_term_freq_df = pd.DataFrame(term_freq_df.sort_values(by='count', ascending=False)['count'])\n",
    "    return sorted_term_freq_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b291a4-088e-463e-8a5e-326bcb0d9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       count  proportion\n",
      "word                    \n",
      "the   186019    0.050628\n",
      "and   107893    0.029365\n",
      "to    103090    0.028058\n",
      "of     89417    0.024336\n",
      "a      81307    0.022129\n",
      "             count    proportion\n",
      "word                            \n",
      "interestel       1  2.721674e-07\n",
      "interethnic      1  2.721674e-07\n",
      "interfaced       1  2.721674e-07\n",
      "interfacing      1  2.721674e-07\n",
      "ðŒ¼ðŒ¿ðŒ½ðŒ³ðƒ            1  2.721674e-07\n"
     ]
    }
   ],
   "source": [
    "term_df, df = count_vocab_frequencies(df)\n",
    "# p(word).  Note that multiple occurrences of a word in a sentence increases its probability.\n",
    "# We may want to do something about that.\n",
    "term_df['proportion'] = term_df['count']/float(sum(term_df['count']))\n",
    "# Sanity check\n",
    "print(term_df.head())\n",
    "print(term_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8202c12e-6d02-4dbc-a896-c1ce1d3b34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PMI(df_coo, subgroup):\n",
    "    # PMI(x;y) = h(y) - h(y|x)\n",
    "    #          = h(subgroup) - h(subgroup|word)\n",
    "    #          = log (p(subgroup|word) / p(subgroup))\n",
    "    # nPMI additionally divides by -log(p(x,y)) = -log(p(x|y)p(y))\n",
    "\n",
    "    # Calculation of p(subgroup)\n",
    "    subgroup_prob = term_df.loc[subgroup]['proportion']\n",
    "    # Apply a function to all words to calculate log p(subgroup|word)\n",
    "    # The word is indexed by mlb.classes_ ; \n",
    "    # we pull out the word using the mlb.classes_ index and then get its count using our main term_df\n",
    "    pmi_df = pd.DataFrame(df_coo.apply(lambda x: np.log(x.values/term_df.loc[mlb.classes_[x.index]]['count']/subgroup_prob)))\n",
    "    # If all went well, this will be correlated with high frequency words\n",
    "    # Until normalizing\n",
    "    return pmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c6626f-ce6d-4630-a92b-ba89882743ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nPMI(pmi_df, df_coo):\n",
    "    normalize_df = pd.DataFrame(df_coo.apply(lambda x: -np.log(x.values/term_df.loc[mlb.classes_[x.index]]['count'] * term_df.loc[mlb.classes_[x.index]]['proportion'])))\n",
    "    npmi_df = pmi_df/normalize_df\n",
    "    return npmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7207c16-7f2c-470a-b9d7-6a52e03b6a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasha/miniconda3/envs/datametrics/lib/python3.8/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Makes a sparse vector (shape: # sentences x # words),\n",
    "# with the count of each word per sentence.\n",
    "#    the a cat poop woman Blind\n",
    "# 0 \n",
    "# 1\n",
    "mlb = MultiLabelBinarizer()\n",
    "df_mlb = pd.DataFrame(mlb.fit_transform(df['tokenized']))\n",
    "npmi_df_pair = pd.DataFrame(columns=[subgroup1, subgroup2])\n",
    "pmi_df_pair = pd.DataFrame(columns=[subgroup1, subgroup2])\n",
    "for subgroup in (subgroup1, subgroup2):\n",
    "    # Index of the subgroup word in the sparse vector\n",
    "    subgroup_idx = np.where(mlb.classes_ == subgroup)[0][0]\n",
    "    # Dataframe for the subgroup (with counts)\n",
    "    df_subgroup = df_mlb.iloc[:, subgroup_idx]\n",
    "    # Create cooccurence matrix for the given subgroup and all other words.\n",
    "    # Note it also includes the word itself, so that count should be subtracted \n",
    "    # (the word will always co-occur with itself)\n",
    "    df_coo = pd.DataFrame(df_mlb.T.dot(df_subgroup))#.drop(index=subgroup_idx, axis=1)\n",
    "    pmi_df = get_PMI(df_coo, subgroup)\n",
    "    pmi_df_pair[subgroup] = pmi_df\n",
    "    npmi_df = get_nPMI(pmi_df, df_coo)\n",
    "    npmi_df_pair[subgroup] = npmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d670174e-61b4-4af8-9203-540ec928dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# woman - man: If it's negative, it's man-biased; if it's positive, it's woman positive.\n",
    "npmi_bias = pd.DataFrame(npmi_df_pair[subgroup1] - npmi_df_pair[subgroup2])\n",
    "pmi_bias = pd.DataFrame(pmi_df_pair[subgroup1] - pmi_df_pair[subgroup2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "986c4cb3-ba23-4bcd-8f08-1d17c01a7f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words that only occur with one or the other -- needed for PMI\n",
    "s1_only_words = pmi_bias[pmi_bias[0].values==np.inf]\n",
    "s2_only_words = pmi_bias[pmi_bias[0].values==-np.inf]\n",
    "\n",
    "# Filter\n",
    "npmi_bias_filtered = npmi_bias.dropna() # [(np.inf > npmi_bias[0]) & (npmi_bias[0] > -np.inf)].sort_values(by=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63bedd3c-6ae4-44e5-a063-80d943efbf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd94085-e77e-4fe3-a955-1821f54426de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@50, the man bias is:\t0.28\n",
      "@50, the woman bias is:\t2.54\n"
     ]
    }
   ],
   "source": [
    "print(\"@%s, the %s bias is:\\t%.2f\" % (n, subgroup2, np.abs(sum(npmi_bias_filtered[:n].values))))\n",
    "print(\"@%s, the %s bias is:\\t%.2f\" % (n, subgroup1, sum(npmi_bias_filtered[-n:].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec116a-29d5-4b88-8aac-cd3ced1aa8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top %s most %s-biased words\" % (n, subgroup2))\n",
    "npmi_bias_filtered.sort_values(by=[0], ascending=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb965a-4709-4d03-b682-bc8c1af41175",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top %s most %s-biased words\" % (n,subgroup1))\n",
    "npmi_bias_filtered.sort_values(by=[0], ascending=True)[-n:].sort_values(by=[0], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2051534c-9486-4cef-9411-d678bb651df1",
   "metadata": {},
   "source": [
    "## Trying out Spacy + WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fc82c164-8844-48eb-b5de-145367377638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_wordnet.wordnet_annotator.WordnetAnnotator at 0x7fd4e7c18be0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"spacy_wordnet\", after='tagger', config={'lang': nlp.lang})\n",
    "#nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b2467bd-d8df-4222-a0d5-509d2418e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "womanwords=npmi_bias_filtered.sort_values(by=[0], ascending=True)[-n:].sort_values(by=[0], ascending=False).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "655e59e5-8098-43f6-af38-e5de99b336cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Docs: https://github.com/recognai/spacy-wordnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fd529c55-eedb-44e9-afe6-aad291acd463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman\n",
      "woman.n.01\n",
      "woman.n.02\n",
      "charwoman.n.01\n",
      "womanhood.n.02\n",
      "quit\n",
      "discontinue.v.01\n",
      "leave_office.v.01\n",
      "depart.v.04\n",
      "foreswear.v.02\n",
      "drop_out.v.01\n",
      "swedish\n",
      "swedish.n.01\n",
      "swedish.a.01\n",
      "complement\n",
      "complement.n.01\n",
      "complement.n.02\n",
      "complement.n.03\n",
      "complement.n.04\n",
      "complement.n.05\n",
      "complement.n.06\n",
      "complement.v.01\n",
      "vitamins\n",
      "vitamin.n.01\n"
     ]
    }
   ],
   "source": [
    "for token in womanwords[:5]:\n",
    "    print(token)\n",
    "    # We get those synsets within the desired domains\n",
    "    token= nlp(token)[0]\n",
    "    synsets = token._.wordnet.synsets()\n",
    "    for s in synsets:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ff2bca7d-7ad5-48ff-9990-dc14401fb0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'shoot': 14, 'representation': 9, 'flight': 9, 'down': 8, 'complement': 7, 'chatter': 6, 'chamber': 5, 'fear': 4, 'booster': 4, 'scarf': 4, 'overlap': 4, 'coaster': 3, 'murray': 3, 'helpless': 3, 'birth': 3, 'ritual': 3, 'woman': 2, 'swedish': 2, 'inject': 2, 'dress': 2, 'strengthening': 2, 'strengthen': 2, 'scourge': 2, 'testify': 2, 'dumb': 2, 'cornell': 2, 'epidemic': 2, 'charwoman': 1, 'womanhood': 1, 'discontinue': 1, 'leave_office': 1, 'depart': 1, 'foreswear': 1, 'drop_out': 1, 'vitamin': 1, 'theatrical_performance': 1, 'miraculously': 1, 'activism': 1, 'blast': 1, 'film': 1, 'dart': 1, 'tear': 1, 'photograph': 1, 'fritter': 1, 'childbirth': 1, 'training': 1, 'dressing': 1, 'prepare': 1, 'groom': 1, 'tone': 1, 'malaysia': 1, 'workplace': 1, 'escape': 1, 'trajectory': 1, 'fledge': 1, 'urgently': 1, 'desperately': 1, 'bane': 1, 'terror': 1, 'flagellate': 1, 'lay_waste_to': 1, 'reverence': 1, 'alternately': 1, 'preconceive': 1, 'preconceived': 1, 'parturition': 1, 'parentage': 1, 'give_birth': 1, 'yak': 1, 'chew_the_fat': 1, 'pregnancy': 1, 'brave': 1, 'invest': 1, 'clothe': 1, 'clothed': 1, 'clad': 1, 'honorary': 1, 'dense': 1, 'speechless': 1, 'vastly': 1, 'ball': 1, 'swelling': 1, 'lout': 1, 'hunk': 1, 'lump': 1, 'collocate': 1, 'supporter': 1, 'promoter': 1, 'chester': 1, 'attire': 1, 'overdress': 1, 'brighten': 1, 'clear_up': 1, 'scarf_joint': 1, 'toss_off': 1, 'devour': 1, 'polish': 1, 'lap': 1, 'chambers': 1, 'bedroom': 1, 'doll': 1, 'dame': 1, 'research_worker': 1, 'investigator': 1, 'detective': 1})\n"
     ]
    }
   ],
   "source": [
    "syns=[]\n",
    "for token in womanwords:\n",
    "    # We get those synsets within the desired domains\n",
    "    token= nlp(token)[0]\n",
    "    synsets = token._.wordnet.synsets()\n",
    "    for s in synsets:\n",
    "        syns.append(s.name().split('.')[0])\n",
    "        \n",
    "print(Counter(syns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datametrics",
   "language": "python",
   "name": "datametrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
